We must eat regularly in order to stay alive. Our body transforms the food we eat into the energy which will make
us active and help us to grow. All growing things need energy.
And the largest source of energy is the sun. The plants and animals which form our food were themselves all helped to grow by the sunlight.
After we have eaten them, they are changed into easily dissolved, nutritious substances inside our bodies.
These are then transferred to innumerable cells and converted into energy. Then our bodies burn the energy as a stove burns fuel.
In this roundabout way, the sun’s energy reaches our bodies.
He tells Janes to do a thing. Then, he changes his mind about the work. He tells Jane to do another work. Then,
he again changes his mind about the work. He tells her to do a different work and again, he changes his mind about that work,
too Jane’s boss does this all the time. He says something, and then, he says something else. Jane does not like the change of mind.
She wants her boss to make a decision. She does not want him to change his mind. Jane says, “This thing is wasting my time!”
Today, Jane made a decision. She will go to his room, talk to her boss about her decision. Jane says:
“I enjoy the work. I work lots of hours. I am very good as a worker. But I cannot work like this. We must work better.
You must tell me things to do and do not change your mind.”
Artificial neural network
From Wikipedia, the free encyclopedia
"Neural network" redirects here. For other uses, see Neural network (disambiguation).
Machine learning and
data mining
Kernel Machine.svg
Problems[show]
Supervised learning
(classification • regression)
[show]
Clustering[show]
Dimensionality reduction[show]
Structured prediction[show]
Anomaly detection[show]
Neural nets[show]
Reinforcement learning[show]
Theory[show]
Machine-learning venues[show]
Related articles[show]
Portal-puzzle.svg Machine learning portal
An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.
Artificial neural networks (ANNs) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains.[1] Such systems "learn" (i.e. progressively improve performance on) tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the results to identify cats in other images. They do this without any a priori knowledge about cats, e.g., that they have fur, tails, whiskers and cat-like faces. Instead, they evolve their own set of relevant characteristics from the learning material that they process.
An ANN is based on a collection of connected units or nodes called artificial neurons (a simplified version of biological neurons in an animal brain). Each connection (a simplified version of a synapse) between artificial neurons can transmit a signal from one to another. The artificial neuron that receives the signal can process it and then signal artificial neurons connected to it.
In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is calculated by a non-linear function of the sum of its inputs. Artificial neurons and connections typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that only if the aggregate signal crosses that threshold is the signal sent. Typically, artificial neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention focused on matching specific tasks, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
Contents  [hide]
1	History
1.1	Hebbian learning
1.2	Backpropagation
1.3	Hardware-based designs
1.4	Contests
1.5	Convolutional networks
2	Models
2.1	Components of an artificial neural network
2.2	Neural networks as functions
2.3	Learning
2.4	Learning paradigms
2.5	Learning algorithms
3	Variants
3.1	Group method of data handling
3.2	Convolutional neural networks
3.3	Long short-term memory
3.4	Deep reservoir computing
3.5	Deep belief networks
3.6	Large memory storage and retrieval neural networks
3.7	Stacked (de-noising) auto-encoders
3.8	Deep stacking networks
3.9	Tensor deep stacking networks
3.10	Spike-and-slab RBMs
3.11	Compound hierarchical-deep models
3.12	Deep predictive coding networks
3.13	Networks with separate memory structures
3.14	Multilayer kernel machine
4	Neural architecture search
5	Use
6	Applications
6.1	Neuroscience
7	Theoretical properties
7.1	Computational power
7.2	Capacity
7.3	Convergence
7.4	Generalization and statistics
8	Criticism
8.1	Training issues
8.2	Theoretical issues
8.3	Hardware issues
8.4	Practical counterexamples to criticisms
8.5	Hybrid approaches
9	Types
10	Gallery
11	See also
12	References
13	Bibliography
14	External links
History[edit]
Warren McCulloch and Walter Pitts[2] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.[3]
Hebbian learning[edit]
In the late 1940s, D.O. Hebb[4] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines.
Farley and Clark[5] (1954) first used computational machines, then called "calculators", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).[6]
Rosenblatt[7] (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time.[8]
In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.[9]
The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling.[10][11][12]
Neural network research stagnated after machine learning research by Minsky and Papert (1969),[13] who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power.
Much of artificial intelligence had focused on high-level (symbolic) models that are processed by using algorithms, characterized for example by expert systems with knowledge embodied in if-then rules, until in the late 1980s research expanded to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a cognitive model.[citation needed]
Backpropagation[edit]
A key trigger for renewed interest in neural networks and learning was Werbos's (1975) backpropagation algorithm that effectively solved the exclusive-or problem and more generally accelerated the training of multi-layer networks. Backpropagation distributed the error term back up through the layers, by modifying the weights at each node.[8]
In the mid-1980s, parallel distributed processing became popular under the name connectionism. Rumelhart and McClelland (1986) described the use of connectionism to simulate neural processes.[14]
Support vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity.
Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as "deep learning".
In 1992, max-pooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognition.[15][16][17] In 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.[18]
The vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs).[19][20] As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.
To overcome this problem, Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.[21] Behnke (2003) relied only on the sign of the gradient (Rprop)[22] on problems such as image reconstruction and face localization.
Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine[23] to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an "ancestral pass") from the top level feature activations.[24][25] In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[26]
Hardware-based designs[edit]
Computational devices were created in CMOS, for both biophysical simulation and neuromorphic computing. Nanodevices[27] for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices).[28] Ciresan and colleagues (2010)[29] in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs makes back-propagation feasible for many-layered feedforward neural networks.