The fox said, “Sorry, sir, you are too big.” The next day, the elephant saw all the animals in the forest running for their lives.
The elephant asked them what the matter was. The bear replied, “There is a tiger in the forest. He’s trying to gobble us all up!
The animals all ran away to hide. The elephant wondered what he could do to save everyone in the forest.
Meanwhile, the tiger kept eating up whoever he could find. The elephant walked up to the tiger and said, “Please, Mr. Tiger, 
do not eat up these poor animals.” “Mind your own business!” growled the tiger.
The elephant has a no choice but to give the tiger a hefty kick. The frightened tiger ran for his life. The elephant 
ambled back into the forest to announce the good news to everyone.
Learning[edit]
See also: Mathematical optimization, Estimation theory, and Machine learning
The possibility of learning has attracted the most interest in neural networks. Given a specific task to solve, and a class of functions {\displaystyle \textstyle F} \textstyle F, learning means using a set of observations to find {\displaystyle \textstyle f^{*}\in F} \textstyle f^{*}\in F which solves the task in some optimal sense.
This entails defining a cost function {\displaystyle \textstyle C:F\rightarrow \mathbb {R} } \textstyle C:F\rightarrow \mathbb {R}  such that, for the optimal solution {\displaystyle \textstyle f^{*}} \textstyle f^{*}, {\displaystyle \textstyle C(f^{*})\leq C(f)} \textstyle C(f^{*})\leq C(f) {\displaystyle \textstyle \forall f\in F} \textstyle \forall f\in F – i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization).
The cost function {\displaystyle \textstyle C} \textstyle C is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.
For applications where the solution is data dependent, the cost must necessarily be a function of the observations, otherwise the model would not relate to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model {\displaystyle \textstyle f} \textstyle f, which minimizes {\displaystyle \textstyle C=E\left[(f(x)-y)^{2}\right]} \textstyle C=E\left[(f(x)-y)^{2}\right], for data pairs {\displaystyle \textstyle (x,y)} \textstyle (x,y) drawn from some distribution {\displaystyle \textstyle {\mathcal {D}}} \textstyle {\mathcal {D}}. In practical situations we would only have {\displaystyle \textstyle N} \textstyle N samples from {\displaystyle \textstyle {\mathcal {D}}} \textstyle {\mathcal {D}} and thus, for the above example, we would only minimize {\displaystyle \textstyle {\hat {C}}={\frac {1}{N}}\sum _{i=1}^{N}(f(x_{i})-y_{i})^{2}} \textstyle {\hat {C}}={\frac {1}{N}}\sum _{i=1}^{N}(f(x_{i})-y_{i})^{2}. Thus, the cost is minimized over a sample of the data rather than the entire distribution.
When {\displaystyle \textstyle N\rightarrow \infty } \textstyle N\rightarrow \infty  some form of online machine learning must be used, where the cost is reduced as each new example is seen. While online machine learning is often used when {\displaystyle \textstyle {\mathcal {D}}} \textstyle {\mathcal {D}} is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.
Choosing a cost function[edit]
While it is possible to define an ad hoc cost function, frequently a particular cost (function) is used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function depends on the task.
Backpropagation[edit]
Main article: Backpropagation
A DNN can be discriminatively trained with the standard backpropagation algorithm. Backpropagation is a method to calculate the gradient of the loss function (produces the cost associated with a given state) with respect to the weights in an ANN.
The basics of continuous backpropagation[10][52][53][54] were derived in the context of control theory by Kelley[55] in 1960 and by Bryson in 1961,[56] using principles of dynamic programming. In 1962, Dreyfus published a simpler derivation based only on the chain rule.[57] Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969.[58][59] In 1970, Linnainmaa finally published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions.[60][61] This corresponds to the modern version of backpropagation which is efficient even when the networks are sparse.[10][52][62][63] In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.[64] In 1974, Werbos mentioned the possibility of applying this principle to ANNs,[65] and in 1982, he applied Linnainmaa's AD method to neural networks in the way that is widely used today.[52][66] In 1986, Rumelhart, Hinton and Williams noted that this method can generate useful internal representations of incoming data in hidden layers of neural networks.[67] In 1993, Wan was the first[10] to win an international pattern recognition contest through backpropagation.[68]
The weight updates of backpropagation can be done via stochastic gradient descent using the following equation:
{\displaystyle w_{ij}(t+1)=w_{ij}(t)+\eta {\frac {\partial C}{\partial w_{ij}}}+\xi (t)} {\displaystyle w_{ij}(t+1)=w_{ij}(t)+\eta {\frac {\partial C}{\partial w_{ij}}}+\xi (t)}
where, {\displaystyle \eta } \eta  is the learning rate, {\displaystyle C} C is the cost (loss) function and {\displaystyle \xi (t)} \xi (t) a stochastic term. The choice of the cost function depends on factors such as the learning type (supervised, unsupervised, reinforcement, etc.) and the activation function. For example, when performing supervised learning on a multiclass classification problem, common choices for the activation function and cost function are the softmax function and cross entropy function, respectively. The softmax function is defined as {\displaystyle p_{j}={\frac {\exp(x_{j})}{\sum _{k}\exp(x_{k})}}} p_{j}={\frac {\exp(x_{j})}{\sum _{k}\exp(x_{k})}} where {\displaystyle p_{j}} p_{j} represents the class probability (output of the unit {\displaystyle j} j) and {\displaystyle x_{j}} x_{j} and {\displaystyle x_{k}} x_{k} represent the total input to units {\displaystyle j} j and {\displaystyle k} k of the same level respectively. Cross entropy is defined as {\displaystyle C=-\sum _{j}d_{j}\log(p_{j})} C=-\sum _{j}d_{j}\log(p_{j}) where {\displaystyle d_{j}} d_{j} represents the target probability for output unit {\displaystyle j} j and {\displaystyle p_{j}} p_{j} is the probability output for {\displaystyle j} j after applying the activation function.[69]
These can be used to output object bounding boxes in the form of a binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can learn features that capture geometric information in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize L2 error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.
Alternatives to backpropagation include Extreme Learning Machines,[70] "No-prop" networks,[71] training without backtracking,[72] "weightless" networks,[73][74] and non-connectionist neural networks.
Learning paradigms[edit]
The three major learning paradigms each correspond to a particular learning task. These are supervised learning, unsupervised learning and reinforcement learning.
Supervised learning[edit]
Supervised learning uses a set of example pairs {\displaystyle (x,y),x\in X,y\in Y}  (x, y), x \in X, y \in Y and the aim is to find a function {\displaystyle f:X\rightarrow Y} {\displaystyle f:X\rightarrow Y} in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.[75]
A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, {\displaystyle f(x)} f(x), and the target value {\displaystyle y} y over all the example pairs. Minimizing this cost using gradient descent for the class of neural networks called multilayer perceptrons (MLP), produces the backpropagation algorithm for training neural networks.
Tasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.
Unsupervised learning[edit]
In unsupervised learning, some data {\displaystyle \textstyle x} \textstyle x is given and the cost function to be minimized, that can be any function of the data {\displaystyle \textstyle x} \textstyle x and the network's output, {\displaystyle \textstyle f} \textstyle f.
The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables).
As a trivial example, consider the model {\displaystyle \textstyle f(x)=a} \textstyle f(x)=a where {\displaystyle \textstyle a} \textstyle a is a constant and the cost {\displaystyle \textstyle C=E[(x-f(x))^{2}]} \textstyle C=E[(x-f(x))^{2}]. Minimizing this cost produces a value of {\displaystyle \textstyle a} \textstyle a that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between {\displaystyle \textstyle x} \textstyle x and {\displaystyle \textstyle f(x)} \textstyle f(x), whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized).
Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.
Reinforcement learning[edit]
See also: Stochastic control
In reinforcement learning, data {\displaystyle \textstyle x} \textstyle x are usually not given, but generated by an agent's interactions with the environment. At each point in time {\displaystyle \textstyle t} \textstyle t, the agent performs an action {\displaystyle \textstyle y_{t}} \textstyle y_{t} and the environment generates an observation {\displaystyle \textstyle x_{t}} \textstyle x_{t} and an instantaneous cost {\displaystyle \textstyle c_{t}} \textstyle c_{t}, according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.
More formally the environment is modeled as a Markov decision process (MDP) with states {\displaystyle \textstyle {s_{1},...,s_{n}}\in S} \textstyle {s_{1},...,s_{n}}\in S and actions {\displaystyle \textstyle {a_{1},...,a_{m}}\in A} \textstyle {a_{1},...,a_{m}}\in A with the following probability distributions: the instantaneous cost distribution {\displaystyle \textstyle P(c_{t}|s_{t})} \textstyle P(c_{t}|s_{t}), the observation distribution {\displaystyle \textstyle P(x_{t}|s_{t})} \textstyle P(x_{t}|s_{t}) and the transition {\displaystyle \textstyle P(s_{t+1}|s_{t},a_{t})} \textstyle P(s_{t+1}|s_{t},a_{t}), while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy (i.e., the MC) that minimizes the cost.
ANNs are frequently used in reinforcement learning as part of the overall algorithm.[76][77] Dynamic programming was coupled with ANNs (giving neurodynamic programming) by Bertsekas and Tsitsiklis[78] and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing,[79] natural resources management[80][81] or medicine[82] because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.
Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.
Convergent recursive learning algorithm[edit]
This is a learning method specially designed for cerebellar model articulation controller (CMAC) neural networks. In 2004 a recursive least squares algorithm was introduced to train CMAC neural network online.[83] This algorithm can converge in one step and update all weights in one step with any new input data. Initially, this algorithm had computational complexity of O(N3). Based on QR decomposition, this recursive learning algorithm was simplified to be O(N).[84]
Learning algorithms[edit]
See also: Machine learning
Training a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.
Most employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. Backpropagation training algorithms fall into three categories:
steepest descent (with variable learning rate and momentum, resilient backpropagation);
quasi-Newton (Broyden-Fletcher-Goldfarb-Shanno, one step secant);
Levenberg-Marquardt and conjugate gradient (Fletcher-Reeves update, Polak-Ribiére update, Powell-Beale restart, scaled conjugate gradient).[85]
Evolutionary methods,[86] gene expression programming,[87] simulated annealing,[88] expectation-maximization, non-parametric methods and particle swarm optimization[89] are other methods for training neural networks.
Variants[edit]
Group method of data handling[edit]
Main article: Group method of data handling
The Group Method of Data Handling (GMDH)[90] features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward multilayer perceptron with eight layers.[91] It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task.[92]
Convolutional neural networks[edit]
Main article: Convolutional neural network
A convolutional neural network (CNN) is a class of deep, feed-forward networks, composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling[16] is often structured via Fukushima's convolutional architecture.[93] This architecture allows CNNs to take advantage of the 2D structure of input data.
CNNs are suitable for processing visual and other two-dimensional data.[94][95] They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate.[96] Examples of applications in computer vision include DeepDream [97] and robot navigation[98].
Long short-term memory[edit]
Main article: Long short-term memory
Long short-term memory (LSTM) networks are RNNs that avoid the vanishing gradient problem.[99] LSTM is normally augmented by recurrent gates called forget gates.[100] LSTM networks prevent backpropagated errors from vanishing or exploding.[19] Instead errors can flow backwards through unlimited numbers of virtual layers in space-unfolded LSTM. That is, LSTM can learn "very deep learning" tasks[10] that require memories of events that happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved.[101] LSTM can handle long delays and signals that have a mix of low and high frequency components.
Stacks of LSTM RNNs[102] trained by Connectionist Temporal Classification (CTC)[103] can find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.
In 2003, LSTM started to become competitive with traditional speech recognizers.[104] In 2007, the combination with CTC achieved first good results on speech data.[105] In 2009, a CTC-trained LSTM was the first RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition.[10][34] In 2014, Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without traditional speech processing methods.[106] LSTM also improved large-vocabulary speech recognition,[107][108] text-to-speech synthesis,[109] for Google Android,[52][110] and photo-real talking heads.[111] In 2015, Google's speech recognition experienced a 49% improvement through CTC-trained LSTM.[112]
LSTM became popular in Natural Language Processing. Unlike previous models based on HMMs and similar concepts, LSTM can learn to recognise context-sensitive languages.[113] LSTM improved machine translation [114][115], language modeling[116] and multilingual language processing.[117] LSTM combined with CNNs improved automatic image captioning.[118]
Deep reservoir computing[edit]
Main article: Reservoir computing
Deep Reservoir Computing and Deep Echo State Networks (deepESNs)[119][120] provide a framework for efficiently trained models for hierarchical processing of temporal data, while enabling the investigation of the inherent role of RNN layered composition.[clarification needed]
Deep belief networks[edit]
Main article: Deep belief network
A restricted Boltzmann machine (RBM) with fully connected visible and hidden units. Note there are no hidden-hidden or visible-visible connections.
A deep belief network (DBN) is a probabilistic, generative model made up of multiple layers of hidden units. It can be considered a composition of simple learning modules that make up each layer.[121]
A DBN can be used to generatively pre-train a DNN by using the learned DBN weights as the initial DNN weights. Backpropagation or other discriminative algorithms can then tune these weights. This is particularly helpful when training data are limited, because poorly initialized weights can significantly hinder model performance. These pre-trained weights are in a region of the weight space that is closer to the optimal weights than were they randomly chosen. This allows for both improved modeling and faster convergence of the fine-tuning phase.[122]